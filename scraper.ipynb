{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: bs4 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: lxml in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (4.9.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mariu\\anaconda3\\envs\\comp0197-pt\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mariu\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mariu\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4\n",
    "!pip install lxml\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import requests  \n",
    "from bs4 import BeautifulSoup, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "match_url_list = read_txt_file(os.getcwd() + '\\data\\Ascension.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to get the list of match of the all VLR Season\n",
    "match_url_list = []\n",
    "\n",
    "source_matchlist = requests.get(url='https://www.vlr.gg/event/matches/449/valorant-champions/?series_id=all').text\n",
    "\n",
    "soup_matchlist = BeautifulSoup(source_matchlist,'lxml')\n",
    "\n",
    "days = soup_matchlist.findAll('div', {'class':'wf-card'})\n",
    "\n",
    "for d in range(1,len(days)):\n",
    "    for i in days[d].findAll(href=True):\n",
    "        match_url_list.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marius.reymauzaize\\AppData\\Local\\Temp\\ipykernel_5404\\3337209969.py:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comment_text = [comment.string.strip() for comment in column.find_all(text=lambda text: isinstance(text, Comment))]\n",
      "C:\\Users\\marius.reymauzaize\\AppData\\Local\\Temp\\ipykernel_5404\\3337209969.py:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comment_text = [comment.string.strip() for comment in column.find_all(text=lambda text: isinstance(text, Comment))]\n",
      "C:\\Users\\marius.reymauzaize\\AppData\\Local\\Temp\\ipykernel_5404\\3337209969.py:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  comment_text = [comment.string.strip() for comment in column.find_all(text=lambda text: isinstance(text, Comment))]\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "match_url_list = ['https://www.vlr.gg/237652/apeks-vs-gentle-mates-champions-tour-2023-emea-ascension-gf/?game=all&tab=economy']\n",
    "match_stats = []\n",
    "row1, row2 = [], []\n",
    "\n",
    "for matchnum in range(len(match_url_list)):\n",
    "    url = match_url_list[matchnum]\n",
    "    \n",
    "    source_match = requests.get(url=url).text\n",
    "    soup_match = BeautifulSoup(source_match, features=\"html.parser\")\n",
    "\n",
    "    stage = soup_match.findAll('div', {'class':'match-header-event-series'})[0].text.strip().split(\":\", 1)[0]\n",
    "\n",
    "    series = soup_match.findAll('div', {'class':'match-header-event-series'})[0].text.strip().split(\"\\n\", 1)[1].strip()\n",
    "\n",
    "    # for table_economy[i] if i%2==0 : the full bank with the round description, otherwise that is the higher level information\n",
    "    # the last one is not useful as it is the overall result so only things interesting are [0,5] for a BO3\n",
    "    table_economy = soup_match.findAll('table', {'class':'wf-table-inset mod-econ'})\n",
    "\n",
    "    for i in range(0, len(table_economy)-1, 2):\n",
    "\n",
    "        map_num = i // 2\n",
    "        map_name = soup_match.findAll('div', {'class':'vm-stats-gamesnav-item js-map-switch'})[map_num].text.strip()[1:].strip()\n",
    "\n",
    "        headers_match = [\"Team Name\", \"Map #\", \"Map Name\", \"Stage\", \"Series\", \"Pistol_Won\", \"Eco\", \"Eco_Won\", \"$\", \"$_Won\", \"$$\", \"$$_Won\", '$$$', '$$$_Won', \"Bank\", \"Buys\"]\n",
    "        df_match = pd.DataFrame(columns=headers_match)\n",
    "\n",
    "        table_economy_general = table_economy[i]\n",
    "        bank = table_economy[i+1]\n",
    "\n",
    "        team1, team2 = get_economy_data(table_economy_general)\n",
    "        banks, buys = get_banking_data(bank)\n",
    "\n",
    "        length = len(df_match)\n",
    "\n",
    "        df_match.loc[length], df_match.loc[length+1] = create_economy_row(team1, team2, banks, buys, series, stage, map_num, map_name)\n",
    "\n",
    "        match_stats.append(df_match)\n",
    "\n",
    "result = pd.concat(match_stats).reset_index(drop=True)\n",
    "result = result.apply(pd.to_numeric, errors='ignore')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping of the performance info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
